{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78490863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bae01b77",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'hot_songs_final.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 12\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#importing datasets with audio features\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m hot_songs \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhot_songs_final.csv\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m     13\u001b[0m not_hot_songs \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot_hot_songs_final.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hot_songs_final.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import colorcet as cc\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "\n",
    "#importing datasets with audio features\n",
    "\n",
    "hot_songs = pd.read_csv('hot_songs_final.csv') \n",
    "not_hot_songs = pd.read_csv('not_hot_songs_final.csv') \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2e953c",
   "metadata": {},
   "source": [
    "## 1/ add hot or not column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3352c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hot_songs['hot_or_not'] = 'H'\n",
    "not_hot_songs['hot_or_not'] = 'N'\n",
    "\n",
    "# display(hot_songs)\n",
    "# print()\n",
    "# display(not_hot_songs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33611554",
   "metadata": {},
   "source": [
    "## 2/ concat both dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0b3abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_or_not = pd.concat([hot_songs, not_hot_songs], ignore_index=True)\n",
    "hot_or_not.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4abb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_or_not = hot_or_not.drop(columns=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc88e0a",
   "metadata": {},
   "source": [
    "## 3/ drop audio features that seems irrelevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6eff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = [\"type\", \"uri\", \"track_href\", \"analysis_url\", \"duration_ms\"]\n",
    "# Drop the specified columns\n",
    "hot_or_not_clean = hot_or_not.drop(columns=columns_to_drop)\n",
    "hot_or_not.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a81905",
   "metadata": {},
   "outputs": [],
   "source": [
    "#droping the ID column to avoid confusion for the scaling & clustering\n",
    "\n",
    "hot_or_not_IDless = hot_or_not_clean.drop(columns=[\"id\",\"hot_or_not\",\"track_name\",\"artists\"])\n",
    "hot_or_not_IDless.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1acbfc8",
   "metadata": {},
   "source": [
    "## 4/ Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a4c370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(hot_or_not_IDless)\n",
    "hot_or_not_scaled = scaler.transform(hot_or_not_IDless)\n",
    "filename = \"hot_or_not_scaler.pickle\" # Path with filename\n",
    "with open(filename, \"wb\") as file:\n",
    "        pickle.dump(scaler,file)\n",
    "        \n",
    "hot_or_not_scaled_df = pd.DataFrame(hot_or_not_scaled, columns = hot_or_not_IDless.columns)\n",
    "\n",
    "display(hot_or_not_scaled_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68969d4",
   "metadata": {},
   "source": [
    "## 5/ test different dimensional reduction technics\n",
    "### a/ Testing PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542049a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(hot_or_not_scaled_df)\n",
    "principal_components = pca.transform(hot_or_not_scaled_df)\n",
    "principal_components_df = pd.DataFrame(principal_components, columns=['PCA_'+ str(i) for i in range(1,hot_or_not_scaled_df.shape[1]+1)])\n",
    "principal_components_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee70ff1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)\n",
    "cumulated_explained_variance_ratio = [sum(pca.explained_variance_ratio_[0:i+1]) for i,value in enumerate(pca.explained_variance_ratio_)]\n",
    "cumulated_explained_variance_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5d150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "ax[0].plot(np.arange(1,13), pca.explained_variance_ratio_)\n",
    "ax[0].set_xlabel(\"Principal component\")\n",
    "ax[0].set_title(\"Variance explained by each Principal Component\")\n",
    "ax[1].plot(np.arange(1,13),cumulated_explained_variance_ratio)\n",
    "ax[1].set_title(\"Acumulated variance explained by Principal Components\")\n",
    "ax[1].set_xlabel(\"Number of Principal components\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c505488",
   "metadata": {},
   "source": [
    "### b/ Testing Isomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d667bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "\n",
    "iso = Isomap(n_neighbors=12, n_components=3)\n",
    "iso.fit(hot_or_not_scaled_df)\n",
    "hot_or_not_isomap_transformed = iso.transform(hot_or_not_scaled_df)\n",
    "hot_or_not_isomap_transformed_df = pd.DataFrame(hot_or_not_isomap_transformed, columns=[\"ISO_1\",\"ISO_2\",\"ISO_3\"])\n",
    "hot_or_not_isomap_transformed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89217be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Replace 'hot_or_not_isomap_transformed_df' with your actual DataFrame\n",
    "x = hot_or_not_isomap_transformed_df['ISO_1']\n",
    "y = hot_or_not_isomap_transformed_df['ISO_2']\n",
    "z = hot_or_not_isomap_transformed_df['ISO_3']\n",
    "\n",
    "ax.scatter(x, y, z)\n",
    "\n",
    "ax.set_xlabel(\"ISO_1\")\n",
    "ax.set_ylabel(\"ISO_2\")\n",
    "ax.set_zlabel(\"ISO_3\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5234cc3",
   "metadata": {},
   "source": [
    "### c/ Testing UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93663cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "reducer = UMAP(n_components=2,random_state=42)\n",
    "reducer.fit(hot_or_not_scaled_df)\n",
    "\n",
    "hot_or_not_umap_transformed = reducer.transform(hot_or_not_scaled_df)\n",
    "hot_or_not_umap_transformed_df = pd.DataFrame(hot_or_not_umap_transformed, columns=[\"UMAP_1\",\"UMAP_2\"])\n",
    "hot_or_not_umap_transformed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752a584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(hot_or_not_umap_transformed_df ['UMAP_1'], hot_or_not_umap_transformed_df ['UMAP_2'])\n",
    "ax.set_xlabel(\"UMAP_1\")\n",
    "ax.set_ylabel(\"UMAP_2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fa62c7",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "## 1/ Using HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f1246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6/ select clustering method most performant for our dataset\n",
    "#from the umap dim reduction, seems that more or less 5 groups appear with sufficient distance and concentration\n",
    "\n",
    "\n",
    "\n",
    "#we are selecting HD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438f3611",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from kneed import KneeLocator\n",
    "\n",
    "from dbcv import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c366ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7/ run it \n",
    "# Creating the clustering model\n",
    "#model = HDBSCAN() # Default values are: eps=0.5, min_samples=5\n",
    "#model = HDBSCAN(min_cluster_size=10,min_samples=5)\n",
    "import os\n",
    "model = HDBSCAN(min_cluster_size=30,min_samples=7)\n",
    "\n",
    "path = \"models/\"\n",
    "    # Check whether the specified path exists or not\n",
    "isExist = os.path.exists(path)\n",
    "if not isExist:\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(path)\n",
    "    print(\"The new directory is created!\")\n",
    "\n",
    "filename = \"HDBSCAN.pkl\" # use a descriptive name for your encoder but keep the \".pkl\" file extension\n",
    "with open(path+filename, \"wb\") as file:\n",
    "    pickle.dump(model, file) # Replace \"variable\" with the name of the variable that contains your transformer\n",
    "\n",
    "# HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that's often used for discovering clusters of varying shapes and sizes in data. The key advantage of HDBSCAN is its ability to handle clusters of different densities. When using HDBSCAN, you may need to adjust several parameters to achieve optimal results. Here are the main parameters of HDBSCAN:\n",
    "# min_samples:\n",
    "# This parameter sets the minimum number of samples required to form a dense region. It determines the smallest cluster that can be formed. Smaller values make clusters more sensitive to noise, while larger values may result in fewer and larger clusters.\n",
    "# min_cluster_size:\n",
    "# It sets the minimum number of points required to form a cluster. Clusters smaller than this size are treated as noise. Adjusting this parameter influences the granularity of the clustering. Larger values lead to larger clusters and potentially more noise.\n",
    "# metric:\n",
    "# This parameter defines the distance metric used for calculating distances between points. Common choices include Euclidean distance, Manhattan distance, or other appropriate distance metrics based on your data.\n",
    "# alpha:\n",
    "# The alpha parameter influences the size of the neighborhood around each point. It's a scaling factor for the neighborhood, determining the number of neighbors a point must have to be considered part of a cluster. Smaller values make clusters more tightly packed.\n",
    "# cluster_selection_method:\n",
    "# This parameter determines the method used to select the final clusters. Options include 'eom' (Excess of Mass), 'leaf' (Cluster hierarchy leaf), or None for no automatic cluster selection.\n",
    "# allow_single_cluster:\n",
    "# If set to True, this parameter allows HDBSCAN to assign all points to a single cluster if no clusters can be found that meet the criteria specified by min_cluster_size and min_samples.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fit model and predict clusters\n",
    "hot_or_not_clustered = model.fit_predict(hot_or_not_umap_transformed_df) # .fit(X_scaled_df)\n",
    "\n",
    "pd.Series(hot_or_not_clustered).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4707b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_or_not_umap_transformed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49103aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbcv_score = round(DBCV(hot_or_not_umap_transformed_df.iloc[:,:2].to_numpy(), hot_or_not_clustered, dist_function=euclidean))\n",
    "print(\"The DBCV score is {:.2f}\".format(dbcv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b08e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_or_not_umap_transformed_df['cluster'] = hot_or_not_clustered\n",
    "hot_or_not_umap_transformed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2294c577",
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_or_not_umap_transformed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffe0650",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating a pallette of hihg contrast colors with as many color as cluster labels\n",
    "#custom_palette = sns.color_palette(cc.glasbey, n_colors=cluster.size)\n",
    "custom_palette = sns.color_palette(cc.glasbey, n_colors=hot_or_not_clustered.size)#n_colors=cluster.size\n",
    "# Modifiying the palette to set the color \"black=(0.,0.,0.)\" to the label \"-1\"\n",
    "#custom_palette = [color if cluster != -1 else (0.,0.,0.) for cluster, color in zip(cluster,custom_palette)]\n",
    "sns.scatterplot(hot_or_not_umap_transformed_df, x=\"UMAP_1\", y=\"UMAP_2\", hue=\"cluster\", palette=custom_palette);\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dc9d52",
   "metadata": {},
   "source": [
    "## Removing the noise: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72376b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de371c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hot_or_not_cluster_df = pd.concat([hot_or_not, hot_or_not_umap_transformed_df['cluster']], axis=1)\n",
    "# hot_or_not_cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b0ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hot_or_not_cluster_df_no_noise = hot_or_not_umap_transformed_df[hot_or_not_umap_transformed_df['cluster'] != -1]\n",
    "# hot_or_not_cluster_df_no_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bd87e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8b7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fit model and predict clusters again\n",
    "# model = HDBSCAN(min_cluster_size=30,min_samples=7)\n",
    "\n",
    "# hot_or_not_cluster_df_no_noise2 = model.fit_predict(hot_or_not_cluster_df_no_noise) # .fit(X_scaled_df)\n",
    "\n",
    "# pd.Series(hot_or_not_cluster_df_no_noise2).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9189d249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hot_or_not_umap_transformed_df['cluster'] = hot_or_not_cluster_df_no_noise2\n",
    "# hot_or_not_umap_transformed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc831061",
   "metadata": {},
   "source": [
    "## 2/ Using KMEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f7b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e08cea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=9, random_state=1234)\n",
    "kmeans.fit(hot_or_not_scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0df1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters2 = kmeans.predict(hot_or_not_scaled_df)\n",
    "clusters2\n",
    "pd.Series(clusters2).value_counts().sort_index() # Number of wines in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7765136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_or_not_umap_transformed_df_kmean['clusters2'] = clusters2\n",
    "#hot_or_not_umap_transformed_df_kmean.drop(columns=['cluster'], inplace=True)\n",
    "\n",
    "hot_or_not_umap_transformed_df_kmean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347ba7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_palette = sns.color_palette(cc.glasbey)\n",
    "# Modifiying the palette to set the color \"black=(0.,0.,0.)\" to the label \"-1\"\n",
    "#custom_palette = [color if cluster != -1 else (0.,0.,0.) for cluster, color in zip(cluster,custom_palette)]\n",
    "sns.scatterplot(data=hot_or_not_umap_transformed_df_kmean, x=\"UMAP_1\", y=\"UMAP_2\", hue=\"clusters2\", palette=custom_palette);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42717c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Silhouette score of the model is: \",round(silhouette_score(hot_or_not_umap_transformed_df_kmean, model.fit_predict(hot_or_not_umap_transformed_df_kmean)),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e82dc2d",
   "metadata": {},
   "source": [
    "### Finding the most relevant \"K\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9634d3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K=9 seems after analysis the best possible input, with a Silhouette score of 0.5 (0.5 and above considered good) and giving sufficient granularity\n",
    "\n",
    "K = range(2, 21)\n",
    "\n",
    "inertia = []\n",
    "silhouette = []\n",
    "\n",
    "for k in K:\n",
    "    print(\"Training a K-Means model with {} clusters! \".format(k))\n",
    "    print()\n",
    "    kmeans = KMeans(n_clusters=k,\n",
    "                    random_state=1234,\n",
    "                    verbose=1)\n",
    "    kmeans.fit(hot_or_not_scaled_df)\n",
    "\n",
    "    filename = \"kmeans_\" + str(k) + \".pickle\" # Path with filename # kmeans_n.pickle\n",
    "    with open(filename, \"wb\") as file:\n",
    "        pickle.dump(kmeans,file)\n",
    "\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    silhouette.append(silhouette_score(hot_or_not_scaled_df, kmeans.predict(hot_or_not_scaled_df)))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(16,8))\n",
    "ax[0].plot(K, inertia, 'bx-')\n",
    "ax[0].set_xlabel('k')\n",
    "ax[0].set_ylabel('inertia')\n",
    "ax[0].set_xticks(np.arange(min(K), max(K)+1, 1.0))\n",
    "ax[0].set_title('Elbow Method showing the optimal k')\n",
    "ax[1].plot(K, silhouette, 'bx-')\n",
    "ax[1].set_xlabel('k')\n",
    "ax[1].set_ylabel('silhouette score')\n",
    "ax[1].set_xticks(np.arange(min(K), max(K)+1, 1.0))\n",
    "ax[1].set_title('Silhouette Method showing the optimal k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b8529d",
   "metadata": {},
   "source": [
    "## Concat hot_or_not df with cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f75c272",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding cluster kmean to the original dataframe\n",
    "hot_or_not[\"cluster_kmean\"] = clusters2\n",
    "hot_or_not.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a411b2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\antho\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - liblapack\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    libblas-3.9.0              |1_h8933c1f_netlib         193 KB  conda-forge\n",
      "    liblapack-3.9.0            |5_hd5c7e75_netlib         2.7 MB  conda-forge\n",
      "    m2w64-gcc-libgfortran-5.3.0|                6         342 KB  conda-forge\n",
      "    m2w64-gcc-libs-5.3.0       |                7         520 KB  conda-forge\n",
      "    m2w64-gcc-libs-core-5.3.0  |                7         214 KB  conda-forge\n",
      "    m2w64-gmp-6.1.0            |                2         726 KB  conda-forge\n",
      "    scikit-learn-1.3.0         |  py311hf62ec03_1         8.1 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        12.8 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  libblas            conda-forge/win-64::libblas-3.9.0-1_h8933c1f_netlib \n",
      "  liblapack          conda-forge/win-64::liblapack-3.9.0-5_hd5c7e75_netlib \n",
      "  m2w64-gcc-libgfor~ conda-forge/win-64::m2w64-gcc-libgfortran-5.3.0-6 \n",
      "  m2w64-gcc-libs     conda-forge/win-64::m2w64-gcc-libs-5.3.0-7 \n",
      "  m2w64-gcc-libs-co~ conda-forge/win-64::m2w64-gcc-libs-core-5.3.0-7 \n",
      "  m2w64-gmp          conda-forge/win-64::m2w64-gmp-6.1.0-2 \n",
      "  scikit-learn       pkgs/main/win-64::scikit-learn-1.3.0-py311hf62ec03_1 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "m2w64-gmp-6.1.0      | 726 KB    |            |   0% \n",
      "\n",
      "liblapack-3.9.0      | 2.7 MB    |            |   0% \u001b[A\n",
      "\n",
      "\n",
      "m2w64-gcc-libs-5.3.0 | 520 KB    |            |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libblas-3.9.0        | 193 KB    |            |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "scikit-learn-1.3.0   | 8.1 MB    |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "m2w64-gcc-libgfortra | 342 KB    |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "m2w64-gcc-libs-core- | 214 KB    |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "scikit-learn-1.3.0   | 8.1 MB    |            |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "m2w64-gcc-libs-5.3.0 | 520 KB    | 3          |   3% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libblas-3.9.0        | 193 KB    | 8          |   8% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "liblapack-3.9.0      | 2.7 MB    |            |   1% \u001b[A\n",
      "m2w64-gmp-6.1.0      | 726 KB    | 2          |   2% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "scikit-learn-1.3.0   | 8.1 MB    | ####8      |  49% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "m2w64-gcc-libgfortra | 342 KB    | 4          |   5% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "liblapack-3.9.0      | 2.7 MB    | ##6        |  26% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libblas-3.9.0        | 193 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libblas-3.9.0        | 193 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "m2w64-gcc-libs-core- | 214 KB    | 7          |   7% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "scikit-learn-1.3.0   | 8.1 MB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "m2w64-gmp-6.1.0      | 726 KB    | #########4 |  95% \n",
      "\n",
      "\n",
      "m2w64-gcc-libs-5.3.0 | 520 KB    | ########## | 100% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "m2w64-gcc-libs-5.3.0 | 520 KB    | ########## | 100% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "m2w64-gcc-libs-core- | 214 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "m2w64-gcc-libgfortra | 342 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "m2w64-gcc-libgfortra | 342 KB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "m2w64-gmp-6.1.0      | 726 KB    | ########## | 100% \n",
      "\n",
      "liblapack-3.9.0      | 2.7 MB    | ########## | 100% \u001b[A\n",
      "\n",
      "liblapack-3.9.0      | 2.7 MB    | ########## | 100% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "scikit-learn-1.3.0   | 8.1 MB    | ########## | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "                                                     \n",
      "\n",
      "\n",
      "                                                     \u001b[A\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... \n",
      "\n",
      "    Windows 64-bit packages of scikit-learn can be accelerated using scikit-learn-intelex.\n",
      "    More details are available here: https://intel.github.io/scikit-learn-intelex\n",
      "\n",
      "    For example:\n",
      "\n",
      "        $ conda install scikit-learn-intelex\n",
      "        $ python -m sklearnex my_application.py\n",
      "\n",
      "\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - conda-forge/noarch::imbalanced-learn==0.11.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::pynndescent==0.5.11=pyhca7485f_0\n",
      "  - defaults/win-64::scikit-learn-intelex==2023.1.1=py311haa95532_0\n",
      "  - conda-forge/win-64::umap-learn==0.5.5=py311h1ea47a8_0\n",
      "  - defaults/win-64::_anaconda_depends==2023.09=py311_mkl_1\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.7.4\n",
      "  latest version: 23.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.11.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Saving the updated dataframe as CSV\n",
    "hot_or_not.to_csv('hot_or_not_clustered.csv', index=False)\n",
    "\n",
    "hot_or_not_umap_transformed_df.to_csv('hot_or_not_umap_transformed_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
